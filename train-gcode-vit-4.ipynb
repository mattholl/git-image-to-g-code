{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in ./venv/lib/python3.12/site-packages (2.3.1)\n",
      "Requirement already satisfied: torchvision in ./venv/lib/python3.12/site-packages (0.18.1)\n",
      "Requirement already satisfied: torchaudio in ./venv/lib/python3.12/site-packages (2.3.1)\n",
      "Requirement already satisfied: filelock in ./venv/lib/python3.12/site-packages (from torch) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./venv/lib/python3.12/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in ./venv/lib/python3.12/site-packages (from torch) (1.12.1)\n",
      "Requirement already satisfied: networkx in ./venv/lib/python3.12/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in ./venv/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in ./venv/lib/python3.12/site-packages (from torch) (2024.5.0)\n",
      "Requirement already satisfied: numpy in ./venv/lib/python3.12/site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in ./venv/lib/python3.12/site-packages (from torchvision) (10.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.12/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in ./venv/lib/python3.12/site-packages (from sympy->torch) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing necessary libraries and packages\n",
    "\n",
    "import requests  # Library for making HTTP requests\n",
    "import torch  # PyTorch for deep learning\n",
    "from PIL import Image  # Library for image processing\n",
    "from transformers import *  # Transformers library for NLP tasks\n",
    "from tqdm import tqdm  # Library for displaying progress bars\n",
    "import numpy as np  # Library for numerical manipulation\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"  # Checking for GPU availability and setting the device accordingly\n",
    "device = torch.device('mps:0') if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "device = \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Encoders and Decoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /Users/matth/.cache/huggingface/hub/models--google--vit-base-patch16-224-in21k/snapshots/b4569560a39a0f1af58e3ddaf17facf20ab919b0/config.json\n",
      "Model config ViTConfig {\n",
      "  \"_name_or_path\": \"google/vit-base-patch16-224-in21k\",\n",
      "  \"architectures\": [\n",
      "    \"ViTModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"encoder_stride\": 16,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"image_size\": 224,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"model_type\": \"vit\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"patch_size\": 16,\n",
      "  \"qkv_bias\": true,\n",
      "  \"transformers_version\": \"4.42.3\"\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /Users/matth/.cache/huggingface/hub/models--google--vit-base-patch16-224-in21k/snapshots/b4569560a39a0f1af58e3ddaf17facf20ab919b0/model.safetensors\n",
      "All model checkpoint weights were used when initializing ViTModel.\n",
      "\n",
      "All the weights of ViTModel were initialized from the model checkpoint at google/vit-base-patch16-224-in21k.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use ViTModel for predictions without further training.\n",
      "loading configuration file config.json from cache at /Users/matth/.cache/huggingface/hub/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"google-bert/bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.42.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "Initializing google-bert/bert-base-uncased as a decoder model. Cross attention layers are added to google-bert/bert-base-uncased and randomly initialized if google-bert/bert-base-uncased's architecture allows for cross attention layers.\n",
      "loading weights file model.safetensors from cache at /Users/matth/.cache/huggingface/hub/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/model.safetensors\n",
      "Generate config GenerationConfig {\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "Some weights of the model checkpoint at google-bert/bert-base-uncased were not used when initializing BertLMHeadModel: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertLMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertLMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertLMHeadModel were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.output.dense.bias', 'bert.encoder.layer.0.crossattention.output.dense.weight', 'bert.encoder.layer.0.crossattention.self.key.bias', 'bert.encoder.layer.0.crossattention.self.key.weight', 'bert.encoder.layer.0.crossattention.self.query.bias', 'bert.encoder.layer.0.crossattention.self.query.weight', 'bert.encoder.layer.0.crossattention.self.value.bias', 'bert.encoder.layer.0.crossattention.self.value.weight', 'bert.encoder.layer.1.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.1.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.1.crossattention.output.dense.bias', 'bert.encoder.layer.1.crossattention.output.dense.weight', 'bert.encoder.layer.1.crossattention.self.key.bias', 'bert.encoder.layer.1.crossattention.self.key.weight', 'bert.encoder.layer.1.crossattention.self.query.bias', 'bert.encoder.layer.1.crossattention.self.query.weight', 'bert.encoder.layer.1.crossattention.self.value.bias', 'bert.encoder.layer.1.crossattention.self.value.weight', 'bert.encoder.layer.10.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.10.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.10.crossattention.output.dense.bias', 'bert.encoder.layer.10.crossattention.output.dense.weight', 'bert.encoder.layer.10.crossattention.self.key.bias', 'bert.encoder.layer.10.crossattention.self.key.weight', 'bert.encoder.layer.10.crossattention.self.query.bias', 'bert.encoder.layer.10.crossattention.self.query.weight', 'bert.encoder.layer.10.crossattention.self.value.bias', 'bert.encoder.layer.10.crossattention.self.value.weight', 'bert.encoder.layer.11.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.11.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.11.crossattention.output.dense.bias', 'bert.encoder.layer.11.crossattention.output.dense.weight', 'bert.encoder.layer.11.crossattention.self.key.bias', 'bert.encoder.layer.11.crossattention.self.key.weight', 'bert.encoder.layer.11.crossattention.self.query.bias', 'bert.encoder.layer.11.crossattention.self.query.weight', 'bert.encoder.layer.11.crossattention.self.value.bias', 'bert.encoder.layer.11.crossattention.self.value.weight', 'bert.encoder.layer.2.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.2.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.2.crossattention.output.dense.bias', 'bert.encoder.layer.2.crossattention.output.dense.weight', 'bert.encoder.layer.2.crossattention.self.key.bias', 'bert.encoder.layer.2.crossattention.self.key.weight', 'bert.encoder.layer.2.crossattention.self.query.bias', 'bert.encoder.layer.2.crossattention.self.query.weight', 'bert.encoder.layer.2.crossattention.self.value.bias', 'bert.encoder.layer.2.crossattention.self.value.weight', 'bert.encoder.layer.3.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.3.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.3.crossattention.output.dense.bias', 'bert.encoder.layer.3.crossattention.output.dense.weight', 'bert.encoder.layer.3.crossattention.self.key.bias', 'bert.encoder.layer.3.crossattention.self.key.weight', 'bert.encoder.layer.3.crossattention.self.query.bias', 'bert.encoder.layer.3.crossattention.self.query.weight', 'bert.encoder.layer.3.crossattention.self.value.bias', 'bert.encoder.layer.3.crossattention.self.value.weight', 'bert.encoder.layer.4.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.4.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.4.crossattention.output.dense.bias', 'bert.encoder.layer.4.crossattention.output.dense.weight', 'bert.encoder.layer.4.crossattention.self.key.bias', 'bert.encoder.layer.4.crossattention.self.key.weight', 'bert.encoder.layer.4.crossattention.self.query.bias', 'bert.encoder.layer.4.crossattention.self.query.weight', 'bert.encoder.layer.4.crossattention.self.value.bias', 'bert.encoder.layer.4.crossattention.self.value.weight', 'bert.encoder.layer.5.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.5.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.5.crossattention.output.dense.bias', 'bert.encoder.layer.5.crossattention.output.dense.weight', 'bert.encoder.layer.5.crossattention.self.key.bias', 'bert.encoder.layer.5.crossattention.self.key.weight', 'bert.encoder.layer.5.crossattention.self.query.bias', 'bert.encoder.layer.5.crossattention.self.query.weight', 'bert.encoder.layer.5.crossattention.self.value.bias', 'bert.encoder.layer.5.crossattention.self.value.weight', 'bert.encoder.layer.6.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.6.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.6.crossattention.output.dense.bias', 'bert.encoder.layer.6.crossattention.output.dense.weight', 'bert.encoder.layer.6.crossattention.self.key.bias', 'bert.encoder.layer.6.crossattention.self.key.weight', 'bert.encoder.layer.6.crossattention.self.query.bias', 'bert.encoder.layer.6.crossattention.self.query.weight', 'bert.encoder.layer.6.crossattention.self.value.bias', 'bert.encoder.layer.6.crossattention.self.value.weight', 'bert.encoder.layer.7.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.7.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.7.crossattention.output.dense.bias', 'bert.encoder.layer.7.crossattention.output.dense.weight', 'bert.encoder.layer.7.crossattention.self.key.bias', 'bert.encoder.layer.7.crossattention.self.key.weight', 'bert.encoder.layer.7.crossattention.self.query.bias', 'bert.encoder.layer.7.crossattention.self.query.weight', 'bert.encoder.layer.7.crossattention.self.value.bias', 'bert.encoder.layer.7.crossattention.self.value.weight', 'bert.encoder.layer.8.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.8.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.8.crossattention.output.dense.bias', 'bert.encoder.layer.8.crossattention.output.dense.weight', 'bert.encoder.layer.8.crossattention.self.key.bias', 'bert.encoder.layer.8.crossattention.self.key.weight', 'bert.encoder.layer.8.crossattention.self.query.bias', 'bert.encoder.layer.8.crossattention.self.query.weight', 'bert.encoder.layer.8.crossattention.self.value.bias', 'bert.encoder.layer.8.crossattention.self.value.weight', 'bert.encoder.layer.9.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.9.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.9.crossattention.output.dense.bias', 'bert.encoder.layer.9.crossattention.output.dense.weight', 'bert.encoder.layer.9.crossattention.self.key.bias', 'bert.encoder.layer.9.crossattention.self.key.weight', 'bert.encoder.layer.9.crossattention.self.query.bias', 'bert.encoder.layer.9.crossattention.self.query.weight', 'bert.encoder.layer.9.crossattention.self.value.bias', 'bert.encoder.layer.9.crossattention.self.value.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Generation config file not found, using a generation config created from the model config.\n",
      "Setting `config.is_decoder=True` and `config.add_cross_attention=True` for decoder_config\n",
      "Generate config GenerationConfig {}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The model used for encoding the image and extracting image features\n",
    "# Available encoder models:\n",
    "# encoder_model = \"WinKawaks/vit-small-patch16-224\"\n",
    "# encoder_model = \"google/vit-base-patch16-224\"\n",
    "encoder_model = \"google/vit-base-patch16-224-in21k\"\n",
    "# encoder_model = \"microsoft/swin-base-patch4-window7-224-in22k\"\n",
    "\n",
    "# The model used for decoding the image features and generating captions\n",
    "# Available decoder models:\n",
    "# decoder_model = \"bert-base-uncased\"\n",
    "# decoder_model = \"prajjwal1/bert-tiny\"\n",
    "# decoder_model = \"gpt2\"\n",
    "decoder_model = \"google-bert/bert-base-uncased\"\n",
    "\n",
    "## Load the pre-trained Encoder and Decoder models\n",
    "model = VisionEncoderDecoderModel.from_encoder_decoder_pretrained(\n",
    "    encoder_model, decoder_model\n",
    ").to(device)\n",
    "# model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a G-code tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['G1</w>', 'X1</w>', '.</w>', '0</w>', 'Y1</w>', '.</w>', '0</w>']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from tokenizers import CharBPETokenizer\n",
    "\n",
    "# Initialize a tokenizer\n",
    "tokenizer = CharBPETokenizer()\n",
    "\n",
    "gcode_dir = \"dataset/gcode\"\n",
    "gcode_files = [os.path.join(gcode_dir, f) for f in os.listdir(gcode_dir) if f.endswith('.txt')]\n",
    "\n",
    "# Then train it!\n",
    "tokenizer.train(gcode_files)\n",
    "\n",
    "# Now, let's use it:\n",
    "encoded = tokenizer.encode(\"G1 X1.0 Y1.0\")\n",
    "\n",
    "# And finally save it somewhere\n",
    "tokenizer.save(\"./bpe-gcode_tokenizer.json\")\n",
    "\n",
    "encoded.tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Tokenizers and Image Processors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file preprocessor_config.json from cache at /Users/matth/.cache/huggingface/hub/models--google--vit-base-patch16-224-in21k/snapshots/b4569560a39a0f1af58e3ddaf17facf20ab919b0/preprocessor_config.json\n",
      "loading configuration file config.json from cache at /Users/matth/.cache/huggingface/hub/models--google--vit-base-patch16-224-in21k/snapshots/b4569560a39a0f1af58e3ddaf17facf20ab919b0/config.json\n",
      "Model config ViTConfig {\n",
      "  \"_name_or_path\": \"google/vit-base-patch16-224-in21k\",\n",
      "  \"architectures\": [\n",
      "    \"ViTModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"encoder_stride\": 16,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"image_size\": 224,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"model_type\": \"vit\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"patch_size\": 16,\n",
      "  \"qkv_bias\": true,\n",
      "  \"transformers_version\": \"4.42.3\"\n",
      "}\n",
      "\n",
      "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
      "loading configuration file preprocessor_config.json from cache at /Users/matth/.cache/huggingface/hub/models--google--vit-base-patch16-224-in21k/snapshots/b4569560a39a0f1af58e3ddaf17facf20ab919b0/preprocessor_config.json\n",
      "size should be a dictionary on of the following set of keys: ({'height', 'width'}, {'shortest_edge'}, {'shortest_edge', 'longest_edge'}, {'longest_edge'}, {'max_width', 'max_height'}), got 224. Converted to {'height': 224, 'width': 224}.\n",
      "Image processor ViTImageProcessor {\n",
      "  \"do_normalize\": true,\n",
      "  \"do_rescale\": true,\n",
      "  \"do_resize\": true,\n",
      "  \"image_mean\": [\n",
      "    0.5,\n",
      "    0.5,\n",
      "    0.5\n",
      "  ],\n",
      "  \"image_processor_type\": \"ViTImageProcessor\",\n",
      "  \"image_std\": [\n",
      "    0.5,\n",
      "    0.5,\n",
      "    0.5\n",
      "  ],\n",
      "  \"resample\": 2,\n",
      "  \"rescale_factor\": 0.00392156862745098,\n",
      "  \"size\": {\n",
      "    \"height\": 224,\n",
      "    \"width\": 224\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Initialize the Tokenizer\n",
    "\n",
    "# The tokenizer is used to preprocess the text and convert it into numerical inputs for the model\n",
    "# Available tokenizers:\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"./bpe-gcode_tokenizer.json\")\n",
    "# tokenizer = GPT2TokenizerFast.from_pretrained(decoder_model)\n",
    "# tokenizer = BertTokenizerFast.from_pretrained(decoder_model)\n",
    "# Initialize the Tokenizer\n",
    "tokenizer = PreTrainedTokenizerFast(tokenizer_file=\"./bpe-gcode_tokenizer.json\")\n",
    "tokenizer.add_special_tokens({'pad_token': '<pad>', 'eos_token': '</s>', 'bos_token': '<s>'})\n",
    "\n",
    "\n",
    "## Initialize the Image Processor\n",
    "\n",
    "# The image processor is used to preprocess the images and extract visual features\n",
    "# Available image processors:\n",
    "# - ViTImageProcessor (for \"google/vit-base-patch16-224\" and \"microsoft/swin-base-patch4-window7-224-in22k\" encoder models)\n",
    "from transformers import AutoImageProcessor\n",
    "\n",
    "# checkpoint = \"google/vit-base-patch16-224-in21k\"\n",
    "image_processor = AutoImageProcessor.from_pretrained(encoder_model)\n",
    "# image_processor = ViTImageProcessor.from_pretrained(encoder_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing and Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "\n",
    "# max_length = 32  # Maximum length of the captions in tokens\n",
    "# coco_dataset_ratio = 50  # 50% of the COCO2014 dataset\n",
    "\n",
    "# # Load the COCO2014 dataset for training, validation, and testing splits\n",
    "# train_ds = load_dataset(\"HuggingFaceM4/COCO\", trust_remote_code=True, split=f\"train[:{coco_dataset_ratio}%]\")\n",
    "# valid_ds = load_dataset(\"HuggingFaceM4/COCO\", trust_remote_code=True, split=f\"validation[:{coco_dataset_ratio}%]\")\n",
    "# test_ds = load_dataset(\"HuggingFaceM4/COCO\", trust_remote_code=True, split=\"test\")\n",
    "\n",
    "# # Get the number of examples in each split\n",
    "# train_len = len(train_ds)\n",
    "# valid_len = len(valid_ds)\n",
    "# test_len = len(test_ds)\n",
    "\n",
    "# train_len, valid_len, test_len  # Display the number of examples in each split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as transforms\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "# Load the custom tokenizer\n",
    "tokenizer = PreTrainedTokenizerFast(tokenizer_file=\"./bpe-gcode_tokenizer.json\")\n",
    "tokenizer.add_special_tokens({'pad_token': '<pad>', 'eos_token': '</s>', 'bos_token': '<s>'})\n",
    "\n",
    "# Dataset class to handle image and G-code pairs\n",
    "class ImageGCodeDataset(Dataset):\n",
    "    def __init__(self, image_dir, gcode_dir, transform=None, tokenizer=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.gcode_dir = gcode_dir\n",
    "        self.transform = transform\n",
    "        self.tokenizer = tokenizer\n",
    "        self.image_files = sorted(os.listdir(image_dir))\n",
    "        self.gcode_files = sorted(os.listdir(gcode_dir))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.image_dir, self.image_files[idx])\n",
    "        gcode_path = os.path.join(self.gcode_dir, self.gcode_files[idx])\n",
    "        \n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        with open(gcode_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            gcode = f.read()\n",
    "\n",
    "        # if self.tokenizer:\n",
    "        #     gcode = self.tokenizer(gcode, return_tensors='pt', padding='max_length', truncation=True, max_length=512)\n",
    "\n",
    "        # return {\"pixel_values\": image, \"labels\": gcode['input_ids'].squeeze()}\n",
    "        return {\"pixel_values\": image, \"labels\": gcode}\n",
    "\n",
    "# Function to load the dataset\n",
    "def load_dataset(image_dir, gcode_dir, tokenizer):\n",
    "    # Define the image transformations\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),  # Resize the images to 224x224 pixels\n",
    "        transforms.ToTensor(),          # Convert the images to PyTorch tensors\n",
    "    ])\n",
    "    \n",
    "    # Create the dataset object\n",
    "    dataset = ImageGCodeDataset(image_dir, gcode_dir, transform, tokenizer)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('./dataset/images', './dataset/gcode', tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.ImageGCodeDataset at 0x317c98a40>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = list(dataset)\n",
    "\n",
    "train_ds = dataset[0:int(len(dataset)*0.5)]\n",
    "valid_ds = dataset[int(len(dataset)*0.5):int(len(dataset)*0.75)]\n",
    "test_ds = dataset[int(len(dataset)*0.75):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pixel_values': tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          ...,\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
       " \n",
       "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          ...,\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
       " \n",
       "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          ...,\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.]]]),\n",
       " 'labels': 'G21 ; Set units to millimeters\\nG90 ; Absolute positioning\\nG1 X4.329 Y3.405 F1200\\nG1 X4.214 Y4.499 F1200\\nG1 X3.114 Y4.479 F1200\\nG1 X2.999 Y3.385 F1200'}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file preprocessor_config.json from cache at /Users/matth/.cache/huggingface/hub/models--google--vit-base-patch16-224-in21k/snapshots/b4569560a39a0f1af58e3ddaf17facf20ab919b0/preprocessor_config.json\n",
      "loading configuration file config.json from cache at /Users/matth/.cache/huggingface/hub/models--google--vit-base-patch16-224-in21k/snapshots/b4569560a39a0f1af58e3ddaf17facf20ab919b0/config.json\n",
      "Model config ViTConfig {\n",
      "  \"_name_or_path\": \"google/vit-base-patch16-224-in21k\",\n",
      "  \"architectures\": [\n",
      "    \"ViTModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"encoder_stride\": 16,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"image_size\": 224,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"model_type\": \"vit\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"patch_size\": 16,\n",
      "  \"qkv_bias\": true,\n",
      "  \"transformers_version\": \"4.42.3\"\n",
      "}\n",
      "\n",
      "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
      "loading configuration file preprocessor_config.json from cache at /Users/matth/.cache/huggingface/hub/models--google--vit-base-patch16-224-in21k/snapshots/b4569560a39a0f1af58e3ddaf17facf20ab919b0/preprocessor_config.json\n",
      "size should be a dictionary on of the following set of keys: ({'height', 'width'}, {'shortest_edge'}, {'shortest_edge', 'longest_edge'}, {'longest_edge'}, {'max_width', 'max_height'}), got 224. Converted to {'height': 224, 'width': 224}.\n",
      "Image processor ViTImageProcessor {\n",
      "  \"do_normalize\": true,\n",
      "  \"do_rescale\": true,\n",
      "  \"do_resize\": true,\n",
      "  \"image_mean\": [\n",
      "    0.5,\n",
      "    0.5,\n",
      "    0.5\n",
      "  ],\n",
      "  \"image_processor_type\": \"ViTImageProcessor\",\n",
      "  \"image_std\": [\n",
      "    0.5,\n",
      "    0.5,\n",
      "    0.5\n",
      "  ],\n",
      "  \"resample\": 2,\n",
      "  \"rescale_factor\": 0.00392156862745098,\n",
      "  \"size\": {\n",
      "    \"height\": 224,\n",
      "    \"width\": 224\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pixel_values': tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]]]), 'labels': 'G21 ; Set units to millimeters\\nG90 ; Absolute positioning\\nG1 X4.329 Y3.405 F1200\\nG1 X4.214 Y4.499 F1200\\nG1 X3.114 Y4.479 F1200\\nG1 X2.999 Y3.385 F1200'}\n",
      "tensor([[ 68,  51,  91,  98,  87,  95,  69,  51,  99, 100,  56,  63,  47,   6,\n",
      "         152,  58,  47, 299,  57,  56,  63,  47, 385,  60,  47, 610,  57,  56,\n",
      "          59,  47, 243,  60]])\n",
      "tensor([[ 68,  51,  91,  98,  87,  95,  69,  51,  99, 100,  56,  63,  47, 470,\n",
      "          58,  47, 338,  57,  56,  63,  47, 183,  34, 102,  47, 283,  57,  56,\n",
      "          61,  47, 656,  60]])\n",
      "tensor([[ 68,  51,  91,  98,  87,  95,  69,  51,  99, 100,  56,  63,  47, 647,\n",
      "          58,  47, 301,  57,  56, 205,  47, 333,  60,  47, 699,  57,  56, 212,\n",
      "          47, 357, 208,  47]])\n",
      "tensor([[ 68,  51,  91,  98,  87,  95,  69,  51,  99, 100,  56, 205,  47,  10,\n",
      "         182,  58,  47, 667,  57,  56, 209,  47, 250,  60,  47, 304,  57,  56,\n",
      "         233,  47, 599,  60]])\n",
      "tensor([[ 68,  51,  91,  98,  87,  95,  69,  51,  99, 100,  56, 103,  47, 704,\n",
      "          58,  47, 253,  57,  56, 103,  47, 188,  35, 102,  47, 549,  57,  56,\n",
      "          59,  47, 162,  41]])\n",
      "tensor([[ 68,  51,  91,  98,  87,  95,  69,  51,  99, 100,  56,  59,  47, 441,\n",
      "          60,  47, 268,  57,  56,  59,  47, 517,  60,  47, 516,  57,  56,  59,\n",
      "          47, 727,  60,  47]])\n",
      "tensor([[ 68,  51,  91,  98,  87,  95,  69,  51,  99, 100,  56, 207,  47,   8,\n",
      "         171,  64,  47, 239,  57,  56, 205,  47, 271,  60,  47, 299,  57,  56,\n",
      "         103,  47, 271,  60]])\n",
      "tensor([[ 68,  51,  91,  98,  87,  95,  69,  51,  99, 100,  56, 103,  47, 112,\n",
      "          38,  58,  47, 261,  57,  56, 167,  47, 290,  64,  47, 495,  57,  56,\n",
      "         209,  47, 147,  35]])\n",
      "tensor([[ 68,  51,  91,  98,  87,  95,  69,  51,  99, 100,  56,  63,  47, 121,\n",
      "          33,  58,  47, 370,  57,  56,  63,  47, 381,  60,  47, 172,  34,  57,\n",
      "          56,  61,  47, 314]])\n",
      "tensor([[ 68,  51,  91,  98,  87,  95,  69,  51,  99, 100,  56, 103,  47, 265,\n",
      "          58,  47, 225,  57,  56, 103,  47, 439,  60,  47, 255,  57,  56,  63,\n",
      "          47, 246,  60,  47]])\n",
      "tensor([[ 68,  51,  91,  98,  87,  95,  69,  51,  99, 100,  56, 205,  47, 540,\n",
      "          58,  47, 332,  57,  56, 167,  47, 493,  64,  47, 322,  57,  56, 214,\n",
      "          47, 621, 200,  47]])\n",
      "tensor([[ 68,  51,  91,  98,  87,  95,  69,  51,  99, 100,  56,  63,  47, 242,\n",
      "          64,  47, 275,  57,  56,  63,  47, 758,  60,  47, 476,  57,  56,  61,\n",
      "          47, 603,  60,  47]])\n",
      "tensor([[ 68,  51,  91,  98,  87,  95,  69,  51,  99, 100,  56,  59,  47, 225,\n",
      "          60,  47,   8, 154,  57,  56,  59,  47, 334,  60,  47, 481,  57,  56,\n",
      "          59,  47, 755,  60]])\n",
      "tensor([[ 68,  51,  91,  98,  87,  95,  69,  51,  99, 100,  56,  63,  47, 414,\n",
      "          64,  47, 501,  57,  56,  63,  47, 462,  58,  47, 188,  36,  57,  56,\n",
      "          59,  47, 617,  58]])\n",
      "tensor([[ 68,  51,  91,  98,  87,  95,  69,  51,  99, 100,  56, 103,  47, 775,\n",
      "          58,  47, 399,  57,  56, 167,  47, 525,  58,  47, 482,  57,  56, 209,\n",
      "          47,   9, 182,  58]])\n",
      "tensor([[ 68,  51,  91,  98,  87,  95,  69,  51,  99, 100,  56,  59,  47, 682,\n",
      "          60,  47,   6, 154,  57,  56,  59,  47, 431,  60,  47, 745,  57,  56,\n",
      "          59,  47, 464,  60]])\n",
      "tensor([[ 68,  51,  91,  98,  87,  95,  69,  51,  99, 100,  56,  61,  47, 468,\n",
      "          60,  47, 281,  57,  56,  61,  47, 737,  60,  47, 802,  57,  56,  61,\n",
      "          47, 305,  60,  47]])\n",
      "tensor([[ 68,  51,  91,  98,  87,  95,  69,  51,  99, 100,  56,  63,  47, 511,\n",
      "          58,  47, 350,  57,  56, 103,  47, 263,  60,  47, 266,  57,  56,  63,\n",
      "          47,   4, 173, 102]])\n",
      "tensor([[ 68,  51,  91,  98,  87,  95,  69,  51,  99, 100,  56, 103,  47, 697,\n",
      "          58,  47, 374,  57,  56,  63,  47, 436,  60,  47, 520,  57,  56,  59,\n",
      "          47, 642,  58,  47]])\n",
      "tensor([[ 68,  51,  91,  98,  87,  95,  69,  51,  99, 100,  56,  59,  47,   4,\n",
      "         175,  60,  47, 627,  57,  56,  61,  47, 219,  40,  60,  47, 607,  57,\n",
      "          56,  61,  47, 346]])\n",
      "tensor([[ 68,  51,  91,  98,  87,  95,  69,  51,  99, 100,  56,  63,  47, 396,\n",
      "          60,  47, 637,  57,  56, 207,  47, 572, 102,  47, 230,  57,  56, 228,\n",
      "          47, 466, 326,  47]])\n",
      "tensor([[ 68,  51,  91,  98,  87,  95,  69,  51,  99, 100,  56, 103,  47, 230,\n",
      "          64,  47, 313,  57,  56,  63,  47, 275,  60,  47, 330,  57,  56,  59,\n",
      "          47, 335,  58,  47]])\n",
      "tensor([[ 68,  51,  91,  98,  87,  95,  69,  51,  99, 100,  56,  59,  47, 552,\n",
      "          64,  47, 360,  57,  56,  63,  47, 610,  58,  47, 162,  36,  57,  56,\n",
      "          59,  47, 369,  60]])\n",
      "tensor([[ 68,  51,  91,  98,  87,  95,  69,  51,  99, 100,  56,  61,  47, 430,\n",
      "          60,  47, 460,  57,  56,  61,  47, 504,  60,  47, 222,  57,  56,  61,\n",
      "          47, 542,  60,  47]])\n",
      "tensor([[ 68,  51,  91,  98,  87,  95,  69,  51,  99, 100,  56,  63,  47, 241,\n",
      "          58,  47, 644,  57,  56,  63,  47, 160,  34, 102,  47, 294,  57,  56,\n",
      "          61,  47, 499, 208]])\n",
      "tensor([[ 68,  51,  91,  98,  87,  95,  69,  51,  99, 100,  56, 205,  47, 350,\n",
      "          64,  47, 779,  57,  56, 167,  47, 316, 208,  47, 736,  57,  56, 214,\n",
      "          47, 346, 326,  47]])\n",
      "tensor([[ 68,  51,  91,  98,  87,  95,  69,  51,  99, 100,  56,  63,  47,   8,\n",
      "         185,  60,  47, 198,  34,  57,  56,  63,  47, 624, 102,  47, 759,  57,\n",
      "          56,  59,  47, 124]])\n",
      "tensor([[ 68,  51,  91,  98,  87,  95,  69,  51,  99, 100,  56, 207,  47, 598,\n",
      "          64,  47, 429,  57,  56, 212,  47, 434,  64,  47, 256,  57,  56, 202,\n",
      "          47, 238,  43,  50]])\n",
      "tensor([[ 68,  51,  91,  98,  87,  95,  69,  51,  99, 100,  56,  63,  47, 602,\n",
      "          64,  47, 140,  36,  57,  56, 207,  47,   6, 127, 229,  47, 165,  37,\n",
      "          57,  56, 167,  47]])\n",
      "tensor([[ 68,  51,  91,  98,  87,  95,  69,  51,  99, 100,  56,  59,  47, 268,\n",
      "          64,  47, 190,  36,  57,  56,  63,  47, 373,  60,  47, 614,  57,  56,\n",
      "          61,  47, 306,  60]])\n",
      "tensor([[ 68,  51,  91,  98,  87,  95,  69,  51,  99, 100,  56, 103,  47, 292,\n",
      "          58,  47, 718,  57,  56, 103,  47, 625, 102,  47, 250,  57,  56,  59,\n",
      "          47, 444, 102,  47]])\n",
      "tensor([[ 68,  51,  91,  98,  87,  95,  69,  51,  99, 100,  56, 103,  47, 498,\n",
      "          64,  47, 449,  57,  56, 103,  47, 289,  60,  47, 433,  57,  56,  59,\n",
      "          47,   6, 159, 102]])\n",
      "tensor([[ 68,  51,  91,  98,  87,  95,  69,  51,  99, 100,  56,  63,  47, 698,\n",
      "          58,  47, 194,  34,  57,  56,  59,  47, 219,  37,  60,  47, 309,  57,\n",
      "          56,  61,  47, 260]])\n",
      "tensor([[ 68,  51,  91,  98,  87,  95,  69,  51,  99, 100,  56,  63,  47, 365,\n",
      "          60,  47, 373,  57,  56,  59,  47, 679, 102,  47, 481,  57,  56,  61,\n",
      "          47, 120,  42, 102]])\n",
      "tensor([[ 68,  51,  91,  98,  87,  95,  69,  51,  99, 100,  56, 207,  47, 261,\n",
      "          58,  47, 375,  57,  56, 205,  47, 250, 200,  47, 283,  57,  56, 205,\n",
      "          47, 317,  43,  50]])\n",
      "tensor([[ 68,  51,  91,  98,  87,  95,  69,  51,  99, 100,  56,  63,  47,  10,\n",
      "          65,  58,  47, 513,  57,  56, 103,  47, 603, 102,  47, 390,  57,  56,\n",
      "          59,  47, 398, 102]])\n",
      "tensor([[ 68,  51,  91,  98,  87,  95,  69,  51,  99, 100,  56,  63,  47, 252,\n",
      "          58,  47, 571,  57,  56,  63,  47, 365,  60,  47, 351,  57,  56,  61,\n",
      "          47, 440,  60,  47]])\n",
      "tensor([[ 68,  51,  91,  98,  87,  95,  69,  51,  99, 100,  56,  59,  47, 270,\n",
      "          64,  47, 253,  57,  56,  59,  47, 323,  58,  47, 720,  57,  56,  61,\n",
      "          47, 435,  58,  47]])\n",
      "tensor([[ 68,  51,  91,  98,  87,  95,  69,  51,  99, 100,  56, 103,  47, 304,\n",
      "          58,  47, 766,  57,  56, 205,  47, 601,  64,  47, 567,  57,  56, 167,\n",
      "          47, 216, 200,  47]])\n",
      "tensor([[ 68,  51,  91,  98,  87,  95,  69,  51,  99, 100,  56,  59,  47,   5,\n",
      "         192, 102,  47,   8, 159,  57,  56,  61,  47, 449, 102,  47, 263,  57,\n",
      "          56,  61,  47, 530]])\n",
      "tensor([[ 68,  51,  91,  98,  87,  95,  69,  51,  99, 100,  56, 103,  47, 301,\n",
      "          58,  47, 293,  57,  56, 103,  47, 391, 102,  47,   5, 195,  57,  56,\n",
      "          59,  47, 218,  60]])\n",
      "tensor([[ 68,  51,  91,  98,  87,  95,  69,  51,  99, 100,  56, 207,  47, 377,\n",
      "          64,  47, 456,  57,  56, 205,  47, 505,  64,  47, 266,  57,  56, 209,\n",
      "          47, 183,  37, 102]])\n",
      "tensor([[ 68,  51,  91,  98,  87,  95,  69,  51,  99, 100,  56, 103,  47, 286,\n",
      "          58,  47, 395,  57,  56, 103,  47, 295, 102,  47, 536,  57,  56,  59,\n",
      "          47, 194,  39, 102]])\n",
      "tensor([[ 68,  51,  91,  98,  87,  95,  69,  51,  99, 100,  56,  63,  47, 189,\n",
      "          41,  60,  47, 352,  57,  56,  63,  47, 654, 102,  47, 141,  34,  57,\n",
      "          56,  59,  47, 502]])\n",
      "tensor([[ 68,  51,  91,  98,  87,  95,  69,  51,  99, 100,  56, 103,  47, 701,\n",
      "          64,  47, 734,  57,  56, 103,  47, 540,  60,  47, 344,  57,  56,  59,\n",
      "          47, 739,  58,  47]])\n",
      "tensor([[ 68,  51,  91,  98,  87,  95,  69,  51,  99, 100,  56,  63,  47, 218,\n",
      "          58,  47, 641,  57,  56, 212,  47, 245,  58,  47, 710,  57,  56, 202,\n",
      "          47, 339,  64,  47]])\n",
      "tensor([[ 68,  51,  91,  98,  87,  95,  69,  51,  99, 100,  56,  59,  47, 793,\n",
      "          60,  47, 816,  57,  56,  61,  47, 483,  60,  47, 302,  57,  56,  61,\n",
      "          47, 774,  60,  47]])\n",
      "tensor([[ 68,  51,  91,  98,  87,  95,  69,  51,  99, 100,  56,  63,  47, 113,\n",
      "          38,  58,  47, 546,  57,  56,  63,  47, 600,  60,  47, 161,  33,  57,\n",
      "          56,  59,  47, 593]])\n",
      "tensor([[ 68,  51,  91,  98,  87,  95,  69,  51,  99, 100,  56, 103,  47,   3,\n",
      "         186,  64,  47, 260,  57,  56, 103,  47, 158,  33,  60,  47, 210,  57,\n",
      "          56,  59,  47, 601]])\n",
      "tensor([[ 68,  51,  91,  98,  87,  95,  69,  51,  99, 100,  56,  63,  47, 725,\n",
      "          64,  47, 269,  57,  56,  63,  47, 789,  60,  47, 526,  57,  56,  61,\n",
      "          47, 337,  60,  47]])\n",
      "tensor([[ 68,  51,  91,  98,  87,  95,  69,  51,  99, 100,  56,  59,  47, 535,\n",
      "          58,  47, 539,  57,  56,  59,  47, 488,  60,  47, 363,  57,  56,  61,\n",
      "          47, 675,  60,  47]])\n",
      "tensor([[ 68,  51,  91,  98,  87,  95,  69,  51,  99, 100,  56,  61,  47, 317,\n",
      "          60,  47, 479,  57,  56,  61,  47, 227,  60,  47, 463,  57,  56,  61,\n",
      "          47, 481,  60,  47]])\n",
      "tensor([[ 68,  51,  91,  98,  87,  95,  69,  51,  99, 100,  56, 228,  47, 388,\n",
      "          58,  47,   6, 137,  57,  56, 167,  47, 333,  58,  47, 487,  57,  56,\n",
      "         214,  47, 294,  64]])\n",
      "tensor([[ 68,  51,  91,  98,  87,  95,  69,  51,  99, 100,  56, 103,  47, 522,\n",
      "          60,  47, 571,  57,  56, 103,  47, 297, 102,  47, 305,  57,  56,  59,\n",
      "          47, 255, 102,  47]])\n",
      "tensor([[ 68,  51,  91,  98,  87,  95,  69,  51,  99, 100,  56,  63,  47, 709,\n",
      "          58,  47, 595,  57,  56,  63,  47, 585,  60,  47, 639,  57,  56,  61,\n",
      "          47, 702, 102,  47]])\n",
      "tensor([[ 68,  51,  91,  98,  87,  95,  69,  51,  99, 100,  56,  59,  47, 210,\n",
      "          64,  47, 709,  57,  56,  59,  47, 254,  58,  47, 284,  57,  56,  61,\n",
      "          47, 338,  58,  47]])\n",
      "tensor([[ 68,  51,  91,  98,  87,  95,  69,  51,  99, 100,  56,  59,  47,   5,\n",
      "         206,  60,  47, 359,  57,  56,  59,  47, 795,  60,  47, 392,  57,  56,\n",
      "          61,  47, 752,  60]])\n",
      "tensor([[ 68,  51,  91,  98,  87,  95,  69,  51,  99, 100,  56,  63,  47, 422,\n",
      "          64,  47, 442,  57,  56,  63,  47, 300,  60,  47, 131,  36,  57,  56,\n",
      "          61,  47, 638,  60]])\n",
      "tensor([[ 68,  51,  91,  98,  87,  95,  69,  51,  99, 100,  56,  61,  47, 261,\n",
      "         102,  47, 288,  57,  56,  61,  47, 564, 102,  47, 579,  57,  56, 104,\n",
      "          47, 484, 102,  47]])\n",
      "tensor([[ 68,  51,  91,  98,  87,  95,  69,  51,  99, 100,  56,  61,  47, 438,\n",
      "          60,  47, 773,  57,  56,  61,  47, 784,  60,  47, 310,  57,  56,  61,\n",
      "          47, 395,  60,  47]])\n",
      "tensor([[ 68,  51,  91,  98,  87,  95,  69,  51,  99, 100,  56,  63,  47, 629,\n",
      "          58,  47, 379,  57,  56,  63,  47, 252,  60,  47, 569,  57,  56,  59,\n",
      "          47, 252,  60,  47]])\n",
      "tensor([[ 68,  51,  91,  98,  87,  95,  69,  51,  99, 100,  56,  63,  47,   4,\n",
      "         206,  58,  47, 472,  57,  56,  63,  47, 580,  60,  47, 716,  57,  56,\n",
      "          61,  47, 439,  60]])\n",
      "tensor([[ 68,  51,  91,  98,  87,  95,  69,  51,  99, 100,  56,  59,  47, 630,\n",
      "          58,  47,  11, 159,  57,  56,  59,  47, 147,  33,  58,  47, 478,  57,\n",
      "          56,  59,  47, 536]])\n",
      "tensor([[ 68,  51,  91,  98,  87,  95,  69,  51,  99, 100,  56,  59,  47, 788,\n",
      "          58,  47, 226,  57,  56,  59,  47, 158,  35,  58,  47, 717,  57,  56,\n",
      "          59,  47, 371,  58]])\n",
      "tensor([[ 68,  51,  91,  98,  87,  95,  69,  51,  99, 100,  56,  59,  47, 331,\n",
      "          60,  47, 641,  57,  56,  59,  47, 243,  60,  47, 527,  57,  56,  59,\n",
      "          47, 216,  60,  47]])\n",
      "tensor([[ 68,  51,  91,  98,  87,  95,  69,  51,  99, 100,  56, 103,  47,   5,\n",
      "         173,  64,  47, 807,  57,  56, 207,  47, 541,  64,  47, 479,  57,  56,\n",
      "         167,  47, 728,  58]])\n",
      "tensor([[ 68,  51,  91,  98,  87,  95,  69,  51,  99, 100,  56,  63,  47, 465,\n",
      "          58,  47, 452,  57,  56,  63,  47, 679,  60,  47, 394,  57,  56,  59,\n",
      "          47, 345,  60,  47]])\n",
      "tensor([[ 68,  51,  91,  98,  87,  95,  69,  51,  99, 100,  56,  63,  47, 507,\n",
      "          58,  47, 364,  57,  56,  63,  47, 680, 102,  47, 158,  36,  57,  56,\n",
      "          59,  47, 297, 102]])\n",
      "tensor([[ 68,  51,  91,  98,  87,  95,  69,  51,  99, 100,  56, 103,  47, 747,\n",
      "          58,  47, 473,  57,  56, 103,  47, 210,  60,  47, 627,  57,  56,  63,\n",
      "          47, 455,  60,  47]])\n",
      "tensor([[ 68,  51,  91,  98,  87,  95,  69,  51,  99, 100,  56,  61,  47, 210,\n",
      "          60,  47,   9, 152,  57,  56,  61,  47, 301,  60,  47, 281,  57,  56,\n",
      "          61,  47, 352,  60]])\n",
      "tensor([[ 68,  51,  91,  98,  87,  95,  69,  51,  99, 100,  56,  63,  47, 238,\n",
      "          64,  47,  12, 195,  57,  56,  63,  47, 344,  60,  47,   7, 130,  57,\n",
      "          56,  61,  47, 803]])\n",
      "tensor([[ 68,  51,  91,  98,  87,  95,  69,  51,  99, 100,  56, 207,  47,  11,\n",
      "         125,  58,  47, 569,  57,  56, 212,  47, 506, 200,  47, 397,  57,  56,\n",
      "         202,  47, 672,  43]])\n",
      "tensor([[ 68,  51,  91,  98,  87,  95,  69,  51,  99, 100,  56,  61,  47,  12,\n",
      "         144,  58,  47, 519,  57,  56,  61,  47, 534,  60,  47, 352,  57,  56,\n",
      "         104,  47, 443,  60]])\n",
      "tensor([[ 68,  51,  91,  98,  87,  95,  69,  51,  99, 100,  56,  59,  47, 332,\n",
      "         102,  47, 383,  57,  56,  59,  47, 685, 102,  47, 129,  35,  57,  56,\n",
      "          59,  47, 570, 102]])\n",
      "tensor([[ 68,  51,  91,  98,  87,  95,  69,  51,  99, 100,  56,  63,  47, 248,\n",
      "          64,  47, 260,  57,  56,  63,  47, 292,  58,  47, 724,  57,  56,  59,\n",
      "          47, 519,  60,  47]])\n",
      "tensor([[ 68,  51,  91,  98,  87,  95,  69,  51,  99, 100,  56, 103,  47, 121,\n",
      "          40,  58,  47, 760,  57,  56, 103,  47, 140,  42, 102,  47, 815,  57,\n",
      "          56,  59,  47, 629]])\n",
      "tensor([[ 68,  51,  91,  98,  87,  95,  69,  51,  99, 100,  56,  61,  47, 672,\n",
      "          58,  47, 613,  57,  56,  61,  47, 467,  58,  47, 713,  57,  56,  61,\n",
      "          47, 418,  58,  47]])\n",
      "tensor([[ 68,  51,  91,  98,  87,  95,  69,  51,  99, 100,  56,  63,  47, 619,\n",
      "          58,  47, 729,  57,  56,  59,  47, 489, 102,  47, 799,  57,  56,  61,\n",
      "          47, 600,  60,  47]])\n",
      "tensor([[ 68,  51,  91,  98,  87,  95,  69,  51,  99, 100,  56,  63,  47, 400,\n",
      "          64,  47, 716,  57,  56,  63,  47, 244,  58,  47, 532,  57,  56,  59,\n",
      "          47, 586,  58,  47]])\n",
      "tensor([[ 68,  51,  91,  98,  87,  95,  69,  51,  99, 100,  56,  59,  47, 660,\n",
      "          60,  47, 581,  57,  56,  59,  47, 508, 102,  47,   7, 192,  57,  56,\n",
      "          61,  47, 273, 102]])\n",
      "tensor([[ 68,  51,  91,  98,  87,  95,  69,  51,  99, 100,  56,  59,  47, 258,\n",
      "          58,  47, 782,  57,  56,  59,  47, 303,  60,  47, 788,  57,  56,  61,\n",
      "          47, 303,  60,  47]])\n",
      "tensor([[ 68,  51,  91,  98,  87,  95,  69,  51,  99, 100,  56,  63,  47, 792,\n",
      "          64,  47, 361,  57,  56,  63,  47, 415,  58,  47, 750,  57,  56,  59,\n",
      "          47,   7, 186,  60]])\n",
      "tensor([[ 68,  51,  91,  98,  87,  95,  69,  51,  99, 100,  56,  63,  47, 742,\n",
      "          58,  47, 662,  57,  56, 103,  47, 225, 102,  47, 157,  40,  57,  56,\n",
      "          59,  47, 441, 102]])\n",
      "tensor([[ 68,  51,  91,  98,  87,  95,  69,  51,  99, 100,  56,  63,  47, 319,\n",
      "          58,  47, 424,  57,  56, 103,  47,   6, 150, 102,  47, 525,  57,  56,\n",
      "          59,  47, 657, 208]])\n",
      "tensor([[ 68,  51,  91,  98,  87,  95,  69,  51,  99, 100,  56,  63,  47,  11,\n",
      "         150,  58,  47,   9, 184,  57,  56,  63,  47, 311,  60,  47,   9, 144,\n",
      "          57,  56,  59,  47]])\n",
      "tensor([[ 68,  51,  91,  98,  87,  95,  69,  51,  99, 100,  56,  59,  47, 554,\n",
      "          64,  47, 165,  36,  57,  56,  59,  47, 391,  60,  47, 495,  57,  56,\n",
      "          61,  47, 288,  60]])\n",
      "tensor([[ 68,  51,  91,  98,  87,  95,  69,  51,  99, 100,  56,  59,  47, 392,\n",
      "         102,  47, 249,  57,  56,  59,  47, 340, 102,  47, 169,  39,  57,  56,\n",
      "          59,  47, 455, 102]])\n",
      "tensor([[ 68,  51,  91,  98,  87,  95,  69,  51,  99, 100,  56,  59,  47, 323,\n",
      "          58,  47, 803,  57,  56,  59,  47, 338,  60,  47, 160,  36,  57,  56,\n",
      "          61,  47, 311,  60]])\n",
      "tensor([[ 68,  51,  91,  98,  87,  95,  69,  51,  99, 100,  56, 103,  47, 523,\n",
      "          58,  47, 358,  57,  56, 207,  47, 656,  58,  47, 469,  57,  56, 228,\n",
      "          47,   4, 107, 229]])\n",
      "tensor([[ 68,  51,  91,  98,  87,  95,  69,  51,  99, 100,  56, 205,  47, 321,\n",
      "          64,  47, 674,  57,  56, 212,  47, 646,  64,  47, 674,  57,  56, 202,\n",
      "          47, 223,  58,  47]])\n",
      "tensor([[ 68,  51,  91,  98,  87,  95,  69,  51,  99, 100,  56,  63,  47,  10,\n",
      "         107,  64,  47, 248,  57,  56,  63,  47, 402,  58,  47, 189,  39,  57,\n",
      "          56,  59,  47, 344]])\n",
      "tensor([[ 68,  51,  91,  98,  87,  95,  69,  51,  99, 100,  56,  63,  47, 256,\n",
      "          64,  47, 331,  57,  56,  63,  47, 644,  60,  47, 685,  57,  56,  61,\n",
      "          47, 438,  58,  47]])\n",
      "tensor([[ 68,  51,  91,  98,  87,  95,  69,  51,  99, 100,  56,  59,  47, 216,\n",
      "          60,  47, 285,  57,  56,  61,  47, 259,  58,  47, 446,  57,  56,  61,\n",
      "          47, 411,  58,  47]])\n",
      "tensor([[ 68,  51,  91,  98,  87,  95,  69,  51,  99, 100,  56,  59,  47, 193,\n",
      "          34, 102,  47, 452,  57,  56,  59,  47, 218, 102,  47, 242,  57,  56,\n",
      "          59,  47, 224, 102]])\n",
      "tensor([[ 68,  51,  91,  98,  87,  95,  69,  51,  99, 100,  56,  63,  47, 148,\n",
      "          36,  58,  47, 386,  57,  56,  63,  47, 581,  60,  47, 351,  57,  56,\n",
      "          59,  47, 471,  58]])\n",
      "tensor([[ 68,  51,  91,  98,  87,  95,  69,  51,  99, 100,  56, 103,  47, 262,\n",
      "          58,  47,   3, 118,  57,  56, 228,  47, 393,  60,  47, 546,  57,  56,\n",
      "         212,  47, 262, 102]])\n",
      "tensor([[ 68,  51,  91,  98,  87,  95,  69,  51,  99, 100,  56,  63,  47,   4,\n",
      "         105,  58,  47, 225,  57,  56,  63,  47, 561,  60,  47,   9, 177,  57,\n",
      "          56,  61,  47, 117]])\n",
      "tensor([[ 68,  51,  91,  98,  87,  95,  69,  51,  99, 100,  56,  63,  47, 146,\n",
      "          33,  58,  47, 282,  57,  56,  63,  47, 335,  60,  47, 226,  57,  56,\n",
      "          59,  47, 562,  60]])\n",
      "tensor([[ 68,  51,  91,  98,  87,  95,  69,  51,  99, 100,  56,  63,  47, 634,\n",
      "          58,  47,   4, 196,  57,  56, 205,  47, 798,  64,  47, 394,  57,  56,\n",
      "         228,  47, 280,  60]])\n",
      "tensor([[ 68,  51,  91,  98,  87,  95,  69,  51,  99, 100,  56, 103,  47, 589,\n",
      "          58,  47, 401,  57,  56, 103,  47, 553, 102,  47, 224,  57,  56,  59,\n",
      "          47, 480, 102,  47]])\n",
      "{'pixel_values': tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]]]), 'labels': tensor([[ 68,  51,  91,  98,  87,  95,  69,  51,  99, 100,  56,  63,  47,   6,\n",
      "         152,  58,  47, 299,  57,  56,  63,  47, 385,  60,  47, 610,  57,  56,\n",
      "          59,  47, 243,  60]])}\n"
     ]
    }
   ],
   "source": [
    "max_length = 32\n",
    "\n",
    "# def preprocess(items):\n",
    "#     # Preprocess the image\n",
    "#     pixel_values = image_processor(items[\"pixel_values\"], return_tensors=\"pt\", do_rescale=False).pixel_values.squeeze(0).to(device)\n",
    "\n",
    "\n",
    "#     # Tokenize the captions with truncation and padding\n",
    "#     targets = tokenizer(items[\"labels\"], max_length=max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\").to(device)\n",
    "#     # targets = tokenizer(items[\"labels\"], max_length=max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids.squeeze(0).to(device)\n",
    "\n",
    "#     return {'pixel_values': pixel_values, 'labels': targets[\"input_ids\"]}\n",
    "\n",
    "image_processor = AutoImageProcessor.from_pretrained(encoder_model)\n",
    "\n",
    "def preprocess(items):\n",
    "    \n",
    "    # Preprocess the image\n",
    "    pixel_values = image_processor(items[\"pixel_values\"], return_tensors=\"pt\", do_rescale=False).pixel_values.squeeze(0).float().to(device)\n",
    "    \n",
    "    # Tokenize the captions with truncation and padding\n",
    "    targets = tokenizer(items[\"labels\"], max_length=max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids.to(device)\n",
    "    print(targets)\n",
    "    return {'pixel_values': pixel_values, 'labels': targets}\n",
    "\n",
    "print(train_ds[0])\n",
    "\n",
    "# Apply the preprocess function to transform the datasets during training\n",
    "train_dataset = list(map(preprocess, train_ds))\n",
    "valid_dataset = list(map(preprocess, valid_ds))\n",
    "test_dataset = list(map(preprocess, test_ds))\n",
    "\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pixel_values': tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          ...,\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
       " \n",
       "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          ...,\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
       " \n",
       "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          ...,\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.]]]),\n",
       " 'labels': tensor([[ 68,  51,  91,  98,  87,  95,  69,  51,  99, 100,  56,  59,  47, 535,\n",
       "           58,  47, 539,  57,  56,  59,  47, 488,  60,  47, 363,  57,  56,  61,\n",
       "           47, 675,  60,  47]])}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    preds = eval_pred.predictions\n",
    "    labels = eval_pred.label_ids\n",
    "\n",
    "    # Decode the predictions and labels\n",
    "    pred_str = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    labels_str = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Compute the Rouge score\n",
    "    rouge_result = rouge.compute(predictions=pred_str, references=labels_str)\n",
    "    rouge_result = {k: round(v * 100, 4) for k, v in rouge_result.items()}  # Multiply by 100 to get the same scale as the Rouge score\n",
    "\n",
    "    # Compute the Bleu score\n",
    "    bleu_result = bleu.compute(predictions=pred_str, references=labels_str)\n",
    "\n",
    "    # Get the length of the generated captions\n",
    "    generation_length = bleu_result[\"translation_length\"]\n",
    "\n",
    "    return {\n",
    "        **rouge_result,\n",
    "        \"bleu\": round(bleu_result[\"bleu\"] * 100, 4),\n",
    "        \"gen_len\": bleu_result[\"translation_length\"] / len(preds)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1  # Number of epochs\n",
    "batch_size = 1  # Batch size\n",
    "\n",
    "# Set the number of training epochs and the batch size. Adjust these values \n",
    "# according to your specific requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Example Shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 32])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 32])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 32])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 32])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 32])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 32])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 32])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 32])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 32])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 32])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 32])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 32])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 32])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 32])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 32])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 32])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 32])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 32])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 32])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 32])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 32])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 32])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 32])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 32])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 32])\n",
      "torch.Size([3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "# Iterate over the training dataset and print the shapes of \n",
    "# labels and pixel values for an example.\n",
    "for item in valid_dataset:\n",
    "    print(item[\"labels\"].shape)\n",
    "    print(item[\"pixel_values\"].shape)\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pixel_values': tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          ...,\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
       " \n",
       "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          ...,\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
       " \n",
       "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          ...,\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.]]]),\n",
       " 'labels': tensor([[ 68,  51,  91,  98,  87,  95,  69,  51,  99, 100,  56, 103,  47, 121,\n",
       "           40,  58,  47, 760,  57,  56, 103,  47, 140,  42, 102,  47, 815,  57,\n",
       "           56,  59,  47, 629]])}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Collation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def collate_fn(batch):\n",
    "#     print(batch.shape)\n",
    "#     print(torch.stack([x['pixel_values'] for x in batch]).shape)\n",
    "#     return {\n",
    "#         'pixel_values': torch.stack(torch.tensor([x['pixel_values'] for x in batch])),\n",
    "#         'labels': torch.stack(torch.tensor([x['labels'] for x in batch]))\n",
    "#     }\n",
    "\n",
    "# This function takes a batch of preprocessed examples and stacks the pixel \n",
    "# values and labels into tensors. It will be used by the data loader to collate the samples into batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch Collation Function\n",
    "def collate_fn(batch):\n",
    "    # Extract pixel values and labels\n",
    "    pixel_values = torch.stack([item['pixel_values'] for item in batch]).to(device)\n",
    "    labels = torch.stack([item['labels'] for item in batch]).to(device)\n",
    "\n",
    "    return {\n",
    "        'pixel_values': pixel_values,\n",
    "        'labels': labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "# Define the training arguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    predict_with_generate=True,             # Use generate to calculate the loss\n",
    "    num_train_epochs=num_epochs,            # Number of training epochs\n",
    "    eval_strategy=\"steps\",            # Evaluate after each eval_steps\n",
    "    eval_steps=2000,                        # Evaluate after each 2000 steps\n",
    "    logging_steps=2000,                     # Log after each 2000 steps\n",
    "    save_steps=2000,                        # Save after each 2000 steps\n",
    "    per_device_train_batch_size=batch_size, # Batch size for training\n",
    "    per_device_eval_batch_size=batch_size,  # Batch size for evaluation\n",
    "    output_dir=\"vit-swin-base-224-gpt2-gcode-gen\",  # Output directory for saving checkpoints and logs\n",
    "    # push_to_hub=True # Whether you want to push the model to the hub\n",
    "    # Check this guide for more details: https://huggingface.co/transformers/model_sharing.html\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 224, 224)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_channels, height, width = train_dataset[40]['pixel_values'].shape\n",
    "num_channels, height, width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 32)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "height, width = train_dataset[40]['labels'].shape\n",
    "height, width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "        ...,\n",
       "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "        [1., 1., 1.,  ..., 1., 1., 1.]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[40]['pixel_values'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(model=model, \n",
    "                  args=training_args, \n",
    "                  data_collator=collate_fn,\n",
    "                  train_dataset=train_dataset,\n",
    "                  eval_dataset=valid_dataset,\n",
    "                  tokenizer=tokenizer\n",
    "                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLoader Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def get_eval_loader(eval_dataset=None):\n",
    "    return DataLoader(valid_dataset, collate_fn=collate_fn, batch_size=batch_size)\n",
    "\n",
    "def get_test_loader(eval_dataset=None):\n",
    "    return DataLoader(test_dataset, collate_fn=collate_fn, batch_size=batch_size)\n",
    "\n",
    "# Override the `get_train_dataloader`, `get_eval_dataloader`, and `get_test_dataloader` methods of the trainer\n",
    "# so that we can properly load the data\n",
    "\n",
    "trainer.get_train_dataloader = lambda: DataLoader(train_dataset, collate_fn=collate_fn, batch_size=batch_size)\n",
    "trainer.get_eval_dataloader = get_eval_loader\n",
    "trainer.get_test_dataloader = get_test_loader\n",
    "\n",
    "# These functions define the data loaders for training, evaluation, and testing. \n",
    "# We override the default methods in the trainer to use our custom data loaders \n",
    "# that properly collate the batches using the `collate_fn` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 50\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 50\n",
      "  Number of trainable parameters = 224,270,394\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19296212f9f441539d7c25b3b6083338",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "slow_conv2d_forward_mps: input(device='cpu') and weight(device=mps:0')  must be on the same device",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[78], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Evaluate the model on the test dataset\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# trainer.evaluate(test_dataset)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# If you set the push_to_hub parameter in the TrainingArguments,\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# complete the push to the model hub using the following code\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# trainer.push_to_hub()\u001b[39;00m\n",
      "File \u001b[0;32m~/Code/learning_to_draw_02/venv/lib/python3.12/site-packages/transformers/trainer.py:1932\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1930\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1931\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1932\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1933\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1934\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1935\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1936\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1937\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Code/learning_to_draw_02/venv/lib/python3.12/site-packages/transformers/trainer.py:2268\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2265\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   2267\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 2268\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2270\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2271\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2272\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2273\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2274\u001b[0m ):\n\u001b[1;32m   2275\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2276\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/Code/learning_to_draw_02/venv/lib/python3.12/site-packages/transformers/trainer.py:3307\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   3304\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   3306\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 3307\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3309\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[1;32m   3311\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m~/Code/learning_to_draw_02/venv/lib/python3.12/site-packages/transformers/trainer.py:3338\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   3336\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3337\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 3338\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3339\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   3340\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   3341\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/Code/learning_to_draw_02/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Code/learning_to_draw_02/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Code/learning_to_draw_02/venv/lib/python3.12/site-packages/transformers/models/vision_encoder_decoder/modeling_vision_encoder_decoder.py:586\u001b[0m, in \u001b[0;36mVisionEncoderDecoderModel.forward\u001b[0;34m(self, pixel_values, decoder_input_ids, decoder_attention_mask, encoder_outputs, past_key_values, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m    583\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pixel_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    584\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have to specify pixel_values\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 586\u001b[0m     encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    587\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    589\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    590\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs_encoder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    592\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    593\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(encoder_outputs, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    594\u001b[0m     encoder_outputs \u001b[38;5;241m=\u001b[39m BaseModelOutput(\u001b[38;5;241m*\u001b[39mencoder_outputs)\n",
      "File \u001b[0;32m~/Code/learning_to_draw_02/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Code/learning_to_draw_02/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Code/learning_to_draw_02/venv/lib/python3.12/site-packages/transformers/models/vit/modeling_vit.py:610\u001b[0m, in \u001b[0;36mViTModel.forward\u001b[0;34m(self, pixel_values, bool_masked_pos, head_mask, output_attentions, output_hidden_states, interpolate_pos_encoding, return_dict)\u001b[0m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pixel_values\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m expected_dtype:\n\u001b[1;32m    608\u001b[0m     pixel_values \u001b[38;5;241m=\u001b[39m pixel_values\u001b[38;5;241m.\u001b[39mto(expected_dtype)\n\u001b[0;32m--> 610\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    611\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbool_masked_pos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbool_masked_pos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolate_pos_encoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterpolate_pos_encoding\u001b[49m\n\u001b[1;32m    612\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    614\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(\n\u001b[1;32m    615\u001b[0m     embedding_output,\n\u001b[1;32m    616\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    619\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m    620\u001b[0m )\n\u001b[1;32m    621\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/Code/learning_to_draw_02/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Code/learning_to_draw_02/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Code/learning_to_draw_02/venv/lib/python3.12/site-packages/transformers/models/vit/modeling_vit.py:115\u001b[0m, in \u001b[0;36mViTEmbeddings.forward\u001b[0;34m(self, pixel_values, bool_masked_pos, interpolate_pos_encoding)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    110\u001b[0m     pixel_values: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m    111\u001b[0m     bool_masked_pos: Optional[torch\u001b[38;5;241m.\u001b[39mBoolTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    112\u001b[0m     interpolate_pos_encoding: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    113\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m    114\u001b[0m     batch_size, num_channels, height, width \u001b[38;5;241m=\u001b[39m pixel_values\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m--> 115\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpatch_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolate_pos_encoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterpolate_pos_encoding\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m bool_masked_pos \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    118\u001b[0m         seq_length \u001b[38;5;241m=\u001b[39m embeddings\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/Code/learning_to_draw_02/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Code/learning_to_draw_02/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Code/learning_to_draw_02/venv/lib/python3.12/site-packages/transformers/models/vit/modeling_vit.py:174\u001b[0m, in \u001b[0;36mViTPatchEmbeddings.forward\u001b[0;34m(self, pixel_values, interpolate_pos_encoding)\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m height \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_size[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m width \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_size[\u001b[38;5;241m1\u001b[39m]:\n\u001b[1;32m    170\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    171\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput image size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mheight\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m*\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwidth\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt match model\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    172\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_size[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m*\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_size[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    173\u001b[0m         )\n\u001b[0;32m--> 174\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprojection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mflatten(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m embeddings\n",
      "File \u001b[0;32m~/Code/learning_to_draw_02/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Code/learning_to_draw_02/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Code/learning_to_draw_02/venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Code/learning_to_draw_02/venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: slow_conv2d_forward_mps: input(device='cpu') and weight(device=mps:0')  must be on the same device"
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "\n",
    "# Evaluate the model on the test dataset\n",
    "# trainer.evaluate(test_dataset)\n",
    "\n",
    "# If you set the push_to_hub parameter in the TrainingArguments,\n",
    "# complete the push to the model hub using the following code\n",
    "# trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from transformers import AutoProcessor, VisionEncoderDecoderModel\n",
    "# import requests\n",
    "# from PIL import Image\n",
    "# import torch\n",
    "\n",
    "# processor = AutoProcessor.from_pretrained(\"microsoft/trocr-base-handwritten\")\n",
    "# model = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-base-handwritten\")\n",
    "\n",
    "# # load image from the IAM dataset\n",
    "# url = \"https://fki.tic.heia-fr.ch/static/img/a01-122-02.jpg\"\n",
    "# image = Image.open(requests.get(url, stream=True).raw).convert(\"RGB\")\n",
    "\n",
    "# # training\n",
    "# model.config.decoder_start_token_id = processor.tokenizer.eos_token_id\n",
    "# model.config.pad_token_id = processor.tokenizer.pad_token_id\n",
    "# model.config.vocab_size = model.config.decoder.vocab_size\n",
    "\n",
    "# pixel_values = processor(image, return_tensors=\"pt\").pixel_values\n",
    "# text = \"hello world\"\n",
    "# labels = processor.tokenizer(text, return_tensors=\"pt\").input_ids\n",
    "# outputs = model(pixel_values=pixel_values, labels=labels)\n",
    "# loss = outputs.loss\n",
    "\n",
    "# # inference (generation)\n",
    "# generated_ids = model.generate(pixel_values)\n",
    "# generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, generated_text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
