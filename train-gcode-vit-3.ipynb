{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23b633fd",
   "metadata": {},
   "source": [
    "# Vision Transformer for G-code Generation\n",
    "\n",
    "This notebook demonstrates how to train a Vision Transformer model to generate G-code from images using Hugging Face's ecosystem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f3ead68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in ./venv/lib/python3.12/site-packages (4.42.3)\n",
      "Requirement already satisfied: datasets in ./venv/lib/python3.12/site-packages (2.20.0)\n",
      "Requirement already satisfied: torch in ./venv/lib/python3.12/site-packages (2.3.1)\n",
      "Requirement already satisfied: torchvision in ./venv/lib/python3.12/site-packages (0.18.1)\n",
      "Requirement already satisfied: tokenizers in ./venv/lib/python3.12/site-packages (0.19.1)\n",
      "Requirement already satisfied: filelock in ./venv/lib/python3.12/site-packages (from transformers) (3.15.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in ./venv/lib/python3.12/site-packages (from transformers) (0.23.4)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17 in ./venv/lib/python3.12/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in ./venv/lib/python3.12/site-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./venv/lib/python3.12/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./venv/lib/python3.12/site-packages (from transformers) (2024.5.15)\n",
      "Requirement already satisfied: requests in ./venv/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in ./venv/lib/python3.12/site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./venv/lib/python3.12/site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in ./venv/lib/python3.12/site-packages (from datasets) (16.1.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in ./venv/lib/python3.12/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in ./venv/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in ./venv/lib/python3.12/site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: xxhash in ./venv/lib/python3.12/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in ./venv/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.5.0,>=2023.1.0 in ./venv/lib/python3.12/site-packages (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets) (2024.5.0)\n",
      "Requirement already satisfied: aiohttp in ./venv/lib/python3.12/site-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./venv/lib/python3.12/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in ./venv/lib/python3.12/site-packages (from torch) (1.12.1)\n",
      "Requirement already satisfied: networkx in ./venv/lib/python3.12/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in ./venv/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in ./venv/lib/python3.12/site-packages (from torchvision) (10.3.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./venv/lib/python3.12/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./venv/lib/python3.12/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./venv/lib/python3.12/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./venv/lib/python3.12/site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in ./venv/lib/python3.12/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./venv/lib/python3.12/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.12/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.12/site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.12/site-packages (from requests->transformers) (2024.6.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.12/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./venv/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./venv/lib/python3.12/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./venv/lib/python3.12/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in ./venv/lib/python3.12/site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install necessary libraries\n",
    "!pip install transformers datasets torch torchvision tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c994d10",
   "metadata": {},
   "source": [
    "## Train a Custom Tokenizer\n",
    "\n",
    "Train a custom tokenizer on the G-code dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f5eb86f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tokenizers import Tokenizer, models, pre_tokenizers, decoders, trainers, processors\n",
    "from tokenizers.pre_tokenizers import Split\n",
    "\n",
    "# Train a custom tokenizer\n",
    "def train_gcode_tokenizer(gcode_dir, vocab_size=8000, min_frequency=2, special_tokens=['<s>', '<pad>', '</s>', '<unk>', '<mask>']):\n",
    "    # Initialize a tokenizer\n",
    "    tokenizer = Tokenizer(models.BPE())\n",
    "    \n",
    "    # Pre-tokenizer to split letters and numbers\n",
    "    tokenizer.pre_tokenizer = Split(\n",
    "        pattern=r\"(?<=\\D)(?=\\d)|(?<=\\d)(?=\\D)\",\n",
    "        behavior=\"isolated\",\n",
    "    )\n",
    "    \n",
    "    # Get a list of all G-code files\n",
    "    gcode_files = [os.path.join(gcode_dir, f) for f in os.listdir(gcode_dir) if f.endswith('.txt')]\n",
    "    \n",
    "    # Train the tokenizer\n",
    "    trainer = trainers.BpeTrainer(vocab_size=vocab_size, min_frequency=min_frequency, special_tokens=special_tokens)\n",
    "    tokenizer.train(gcode_files, trainer)\n",
    "    \n",
    "    tokenizer.post_processor = processors.TemplateProcessing(\n",
    "        single=\"<s> $A </s>\",\n",
    "        special_tokens=[\n",
    "            (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
    "            (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
    "        ],\n",
    "    )\n",
    "    tokenizer.decoder = decoders.ByteLevel()\n",
    "    \n",
    "    return tokenizer\n",
    "\n",
    "# Directory containing G-code files\n",
    "gcode_dir = \"dataset/gcode\"  # Replace with the path to your G-code directory\n",
    "\n",
    "# Train and save the tokenizer\n",
    "tokenizer = train_gcode_tokenizer(gcode_dir)\n",
    "tokenizer.save(\"./gcode_tokenizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55f9302",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "Create a dataset class to handle the image and G-code pairs and a function to load the dataset using the custom tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "39891f86",
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "data did not match any variant of untagged enum ModelWrapper at line 1884 column 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PreTrainedTokenizerFast\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Load the custom tokenizer\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mPreTrainedTokenizerFast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenizer_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./gcode_tokenizer\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39madd_special_tokens({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpad_token\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<pad>\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124meos_token\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m</s>\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbos_token\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<s>\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Dataset class to handle image and G-code pairs\u001b[39;00m\n",
      "File \u001b[0;32m~/Code/learning_to_draw_02/venv/lib/python3.12/site-packages/transformers/tokenization_utils_fast.py:115\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m     fast_tokenizer \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(tokenizer_object)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m fast_tokenizer_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m from_slow:\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;66;03m# We have a serialization from tokenizers which let us directly build the backend\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m     fast_tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mTokenizerFast\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfast_tokenizer_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m slow_tokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;66;03m# We need to convert a slow tokenizer to build the backend\u001b[39;00m\n\u001b[1;32m    118\u001b[0m     fast_tokenizer \u001b[38;5;241m=\u001b[39m convert_slow_tokenizer(slow_tokenizer)\n",
      "\u001b[0;31mException\u001b[0m: data did not match any variant of untagged enum ModelWrapper at line 1884 column 3"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as transforms\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "# Load the custom tokenizer\n",
    "tokenizer = PreTrainedTokenizerFast(tokenizer_file=\"./gcode_tokenizer\")\n",
    "tokenizer.add_special_tokens({'pad_token': '<pad>', 'eos_token': '</s>', 'bos_token': '<s>'})\n",
    "\n",
    "# Dataset class to handle image and G-code pairs\n",
    "class ImageGCodeDataset(Dataset):\n",
    "    def __init__(self, image_dir, gcode_dir, transform=None, tokenizer=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.gcode_dir = gcode_dir\n",
    "        self.transform = transform\n",
    "        self.tokenizer = tokenizer\n",
    "        self.image_files = sorted(os.listdir(image_dir))\n",
    "        self.gcode_files = sorted(os.listdir(gcode_dir))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.image_dir, self.image_files[idx])\n",
    "        gcode_path = os.path.join(self.gcode_dir, self.gcode_files[idx])\n",
    "        \n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        with open(gcode_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            gcode = f.read()\n",
    "\n",
    "        if self.tokenizer:\n",
    "            gcode = self.tokenizer(gcode, return_tensors='pt', padding='max_length', truncation=True, max_length=512)\n",
    "\n",
    "        return {\"pixel_values\": image, \"labels\": gcode['input_ids'].squeeze()}\n",
    "\n",
    "# Function to load the dataset\n",
    "def load_dataset(image_dir, gcode_dir, tokenizer):\n",
    "    # Define the image transformations\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),  # Resize the images to 224x224 pixels\n",
    "        transforms.ToTensor(),          # Convert the images to PyTorch tensors\n",
    "    ])\n",
    "    \n",
    "    # Create the dataset object\n",
    "    dataset = ImageGCodeDataset(image_dir, gcode_dir, transform, tokenizer)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890e50a7",
   "metadata": {},
   "source": [
    "## Model Definition\n",
    "\n",
    "Define a Vision Transformer model for image encoding and add a custom head for text (G-code) generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0547e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import VisionEncoderDecoderModel, ViTModel, BertConfig, EncoderDecoderConfig, BertLMHeadModel\n",
    "import torch\n",
    "\n",
    "# Load the vision transformer model\n",
    "encoder = ViTModel.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "\n",
    "# Load the BERT language model\n",
    "decoder_config = BertConfig.from_pretrained(\"bert-base-uncased\")\n",
    "decoder = BertLMHeadModel.from_pretrained(\"bert-base-uncased\", config=decoder_config)\n",
    "\n",
    "# Configuration for the encoder-decoder model\n",
    "config = EncoderDecoderConfig.from_encoder_decoder_configs(encoder.config, decoder.config)\n",
    "config.decoder_start_token_id = tokenizer.cls_token_id\n",
    "config.pad_token_id = tokenizer.pad_token_id\n",
    "config.vocab_size = tokenizer.vocab_size\n",
    "\n",
    "# Define the model\n",
    "model = VisionEncoderDecoderModel(encoder=encoder, decoder=decoder, config=config)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d6fecd",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Define the training loop using Hugging Face's `Trainer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2005edb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# Load dataset\n",
    "image_dir = \"dataset/images\"  # Replace with the path to your image directory\n",
    "gcode_dir = \"dataset/gcode\"   # Replace with the path to your G-code directory\n",
    "dataset = load_dataset(image_dir, gcode_dir, tokenizer)\n",
    "\n",
    "# Split dataset into train and validation\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # Output directory\n",
    "    num_train_epochs=3,              # Total number of training epochs\n",
    "    per_device_train_batch_size=16,  # Batch size for training\n",
    "    per_device_eval_batch_size=16,   # Batch size for evaluation\n",
    "    warmup_steps=500,                # Number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # Strength of weight decay\n",
    "    logging_dir='./logs',            # Directory for storing logs\n",
    "    logging_steps=10,                # Logging steps\n",
    ")\n",
    "\n",
    "# Define a custom collate function\n",
    "def collate_fn(batch):\n",
    "    pixel_values = torch.stack([item['pixel_values'] for item in batch])\n",
    "    labels = torch.stack([item['labels'] for item in batch])\n",
    "    pixel_values = pixel_values.to(device)\n",
    "    labels = labels.to(device)\n",
    "    return {'pixel_values': pixel_values, 'labels': labels}\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,                         # The instantiated ðŸ¤— Transformers model to be trained\n",
    "    args=training_args,                  # Training arguments, defined above\n",
    "    train_dataset=train_dataset,         # Training dataset\n",
    "    eval_dataset=val_dataset,            # Evaluation dataset\n",
    "    data_collator=collate_fn             # Custom data collator to handle device placement\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c0ce75",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Evaluate the model's performance on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c0aac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "results = trainer.evaluate()\n",
    "\n",
    "print(f\"Validation Accuracy: {results['eval_accuracy']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f2a8b5",
   "metadata": {},
   "source": [
    "## Final Remarks\n",
    "\n",
    "This setup provides a basic framework to train a Vision Transformer model to generate G-code from images using Hugging Face's ecosystem. The training script initializes the dataset, defines the model, sets up the trainer, and evaluates the model.\n",
    "\n",
    "You can customize the model architecture, training parameters, and evaluation metrics according to your specific requirements."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
