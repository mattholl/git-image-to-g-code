{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is based on the one found here:\n",
    "\n",
    "https://colab.research.google.com/drive/1ati-phGZWzeX3_Zvy1xgNtcc6PpXgdXk#scrollTo=bon_IYbzq0vz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create image captioning dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import os\n",
    "\n",
    "dataset = load_dataset(\"dataset/dataset_loading_script.py\", data_dir=os.path.abspath(\"./dataset\"), trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': ['file_name', 'text', 'image'],\n",
       " 'validation': ['file_name', 'text', 'image'],\n",
       " 'test': ['file_name', 'text', 'image']}"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCABAAEADASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwD3+iiigAooooAKKKKACiiigAooooAKKKKACivN/F+q6jqbeMbez1Sexs/D2lM7R2zrHNPdNE0qMXVvMWNAExgLvYuCWVSD6RQAUUUUAFFFFABRRRQByfizwbN4geefT9Xk024urKTTrzdEZori3ZXABj3Lh0Z2ZXBBGWB3A4rU0bTNUtLy+vNW1n7fNc+WscMMHkQW6IDwiFnO5izFmLHPyjgKK2KKACiiigAooooAKKKKACiiigAooooA/9k=",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEAAAABACAIAAAAlC+aJAAABC0lEQVR4Ae2YQQ7CMAwECf//cwgEVVW3h0rrbFVpeioWsb0z4ULrvb+e/LyfvPx3dwLcbRADGDAJcIVMgPZxDNgIzQYYMAHaxzFgIzQbYMAEaB/HgI3QbIABE6B9HAM2QrMBBkyA9vEbDLTfY2/+bxANMFev/S8wF2BsP6DVbj8a5gKs2D4XYOIf88qfnIHyyzNZ5AIskhAKsAj/kBAKMHWfSjgtXv+p5AJMCYd1Dx+v7719s62Tu83Yv+jG5gLpAPswJe+5K1SyrjYhgDLJVjCQ5a3TMKBMshUMZHnrNAwok2wFA1neOg0DyiRbwUCWt07DgDLJVjCQ5a3TMKBMshUMZHnrNAwok2zlA9bZHoQjkTl7AAAAAElFTkSuQmCC",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=64x64>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(64, 64)"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = dataset['train'][6]\n",
    "image = example[\"image\"]\n",
    "width, height = image.size\n",
    "display(image)\n",
    "width, height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'G21 ; Set units to millimeters\\nG90 ; Absolute positioning\\nG1 X3.939 Y3.838 F1200\\nG1 X3.885 Y3.830 F1200\\nG1 X3.833 Y3.815 F1200\\nG1 X3.783 Y3.795 F1200\\nG1 X3.735 Y3.770 F1200\\nG1 X3.690 Y3.739 F1200\\nG1 X3.649 Y3.704 F1200\\nG1 X3.611 Y3.665 F1200\\nG1 X3.579 Y3.621 F1200\\nG1 X3.551 Y3.575 F1200\\nG1 X3.528 Y3.526 F1200\\nG1 X3.511 Y3.474 F1200\\nG1 X3.499 Y3.421 F1200\\nG1 X3.494 Y3.367 F1200\\nG1 X3.494 Y3.313 F1200\\nG1 X3.500 Y3.259 F1200\\nG1 X3.512 Y3.206 F1200\\nG1 X3.529 Y3.155 F1200\\nG1 X3.552 Y3.106 F1200\\nG1 X3.580 Y3.059 F1200\\nG1 X3.613 Y3.016 F1200\\nG1 X3.650 Y2.977 F1200\\nG1 X3.692 Y2.942 F1200\\nG1 X3.737 Y2.912 F1200\\nG1 X3.785 Y2.887 F1200\\nG1 X3.835 Y2.867 F1200\\nG1 X3.888 Y2.853 F1200\\nG1 X3.941 Y2.844 F1200\\nG1 X3.995 Y2.841 F1200\\nG1 X4.049 Y2.845 F1200\\nG1 X4.103 Y2.854 F1200\\nG1 X4.155 Y2.868 F1200\\nG1 X4.205 Y2.889 F1200\\nG1 X4.253 Y2.914 F1200\\nG1 X4.298 Y2.945 F1200\\nG1 X4.339 Y2.980 F1200\\nG1 X4.376 Y3.020 F1200'"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create PyTorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class ImageCaptioningDataset(Dataset):\n",
    "    def __init__(self, dataset, processor):\n",
    "        self.dataset = dataset\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "\n",
    "        \n",
    "\n",
    "        encoding = self.processor(images=item[\"image\"], text=item[\"text\"], padding=\"max_length\", max_length=512, truncation=True, return_tensors=\"pt\")\n",
    "        # print(encoding)\n",
    "        # remove batch dimension\n",
    "        encoding = {k:v.squeeze() for k,v in encoding.items()}\n",
    "\n",
    "        return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"microsoft/git-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ImageCaptioningDataset(dataset['train'], processor)\n",
    "\n",
    "test_dataset = ImageCaptioningDataset(dataset['test'], processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  101,  1043, 17465,  1025,  2275,  3197,  2000,  4971, 28136,  1043,\n",
       "        21057,  1025,  7619, 19120,  1043,  2487,  1060,  2509,  1012,  6109,\n",
       "         2683,  1061,  2509,  1012,  6640,  2620, 20069, 28332,  1043,  2487,\n",
       "         1060,  2509,  1012,  6070,  2629,  1061,  2509,  1012,  6640,  2692,\n",
       "        20069, 28332,  1043,  2487,  1060,  2509,  1012,  6640,  2509,  1061,\n",
       "         2509,  1012,  6282,  2629, 20069, 28332,  1043,  2487,  1060,  2509,\n",
       "         1012,  6275,  2509,  1061,  2509,  1012,  6535,  2629, 20069, 28332,\n",
       "         1043,  2487,  1060,  2509,  1012,  6421,  2629,  1061,  2509,  1012,\n",
       "        29065, 20069, 28332,  1043,  2487,  1060,  2509,  1012, 28066,  1061,\n",
       "         2509,  1012,  6421,  2683, 20069, 28332,  1043,  2487,  1060,  2509,\n",
       "         1012,  4185,  2683,  1061,  2509,  1012,  3963,  2549, 20069, 28332,\n",
       "         1043,  2487,  1060,  2509,  1012,  6079,  2487,  1061,  2509,  1012,\n",
       "         5764,  2629, 20069, 28332,  1043,  2487,  1060,  2509,  1012,  5401,\n",
       "         2683,  1061,  2509,  1012,  5786,  2487, 20069, 28332,  1043,  2487,\n",
       "         1060,  2509,  1012,  4583,  2487,  1061,  2509,  1012,  5401,  2629,\n",
       "        20069, 28332,  1043,  2487,  1060,  2509,  1012,  4720,  2620,  1061,\n",
       "         2509,  1012,  4720,  2575, 20069, 28332,  1043,  2487,  1060,  2509,\n",
       "         1012,  4868,  2487,  1061,  2509,  1012,  4700,  2549, 20069, 28332,\n",
       "         1043,  2487,  1060,  2509,  1012,  4749,  2683,  1061,  2509,  1012,\n",
       "        29403, 20069, 28332,  1043,  2487,  1060,  2509,  1012,  4749,  2549,\n",
       "         1061,  2509,  1012,  4029,  2581, 20069, 28332,  1043,  2487,  1060,\n",
       "         2509,  1012,  4749,  2549,  1061,  2509,  1012, 22997, 20069, 28332,\n",
       "         1043,  2487,  1060,  2509,  1012,  3156,  1061,  2509,  1012, 25191,\n",
       "        20069, 28332,  1043,  2487,  1060,  2509,  1012, 24406,  1061,  2509,\n",
       "         1012, 18744, 20069, 28332,  1043,  2487,  1060,  2509,  1012,  4720,\n",
       "         2683,  1061,  2509,  1012, 14168, 20069, 28332,  1043,  2487,  1060,\n",
       "         2509,  1012,  4583,  2475,  1061,  2509,  1012, 10114, 20069, 28332,\n",
       "         1043,  2487,  1060,  2509,  1012, 23712,  1061,  2509,  1012,  5709,\n",
       "         2683, 20069, 28332,  1043,  2487,  1060,  2509,  1012,  6079,  2509,\n",
       "         1061,  2509,  1012,  5890,  2575, 20069, 28332,  1043,  2487,  1060,\n",
       "         2509,  1012, 13757,  1061,  2475,  1012,  5989,  2581, 20069, 28332,\n",
       "         1043,  2487,  1060,  2509,  1012,  6353,  2475,  1061,  2475,  1012,\n",
       "         6365,  2475, 20069, 28332,  1043,  2487,  1060,  2509,  1012, 22061,\n",
       "         1061,  2475,  1012,  6205,  2475, 20069, 28332,  1043,  2487,  1060,\n",
       "         2509,  1012,  6275,  2629,  1061,  2475,  1012,  6070,  2581, 20069,\n",
       "        28332,  1043,  2487,  1060,  2509,  1012,  6640,  2629,  1061,  2475,\n",
       "         1012,  6564,  2581, 20069, 28332,  1043,  2487,  1060,  2509,  1012,\n",
       "         6070,  2620,  1061,  2475,  1012,  5594,  2509, 20069, 28332,  1043,\n",
       "         2487,  1060,  2509,  1012,  6365,  2487,  1061,  2475,  1012,  6391,\n",
       "         2549, 20069, 28332,  1043,  2487,  1060,  2509,  1012,  5585,  2629,\n",
       "         1061,  2475,  1012,  6391,  2487, 20069, 28332,  1043,  2487,  1060,\n",
       "         2549,  1012,  5840,  2683,  1061,  2475,  1012,  6391,  2629, 20069,\n",
       "        28332,  1043,  2487,  1060,  2549,  1012,  9800,  1061,  2475,  1012,\n",
       "         5594,  2549, 20069, 28332,  1043,  2487,  1060,  2549,  1012, 14168,\n",
       "         1061,  2475,  1012,  6564,  2620, 20069, 28332,  1043,  2487,  1060,\n",
       "         2549,  1012, 16327,  1061,  2475,  1012,  6070,  2683, 20069, 28332,\n",
       "         1043,  2487,  1060,  2549,  1012, 23254,  1061,  2475,  1012,  6205,\n",
       "         2549, 20069, 28332,  1043,  2487,  1060,  2549,  1012, 27240,  1061,\n",
       "         2475,  1012,  6365,  2629, 20069, 28332,  1043,  2487,  1060,  2549,\n",
       "         1012, 28977,  1061,  2475,  1012, 25195, 20069, 28332,  1043,  2487,\n",
       "         1060,  2549,  1012,  4261,  2575,  1061,  2509,  1012,  6185,  2692,\n",
       "        20069,   102])"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.__getitem__(6)['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids\n",
      "attention_mask\n",
      "pixel_values\n"
     ]
    }
   ],
   "source": [
    "item = train_dataset[6]\n",
    "for k in item:\n",
    "  # print(k,v.shape)\n",
    "  print(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create PyTorch DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=False, batch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids\n",
      "torch.Size([5, 512])\n",
      "attention_mask\n",
      "torch.Size([5, 512])\n",
      "pixel_values\n",
      "torch.Size([5, 3, 224, 224])\n",
      "input_ids\n",
      "torch.Size([2, 512])\n",
      "attention_mask\n",
      "torch.Size([2, 512])\n",
      "pixel_values\n",
      "torch.Size([2, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "# batch = next(iter(train_dataloader))\n",
    "for batch in train_dataloader:\n",
    "  for k,v in batch.items():\n",
    "    print(k)\n",
    "    print(v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids\n",
      "torch.Size([2, 512])\n",
      "attention_mask\n",
      "torch.Size([2, 512])\n",
      "pixel_values\n",
      "torch.Size([2, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "for k,v in batch.items():\n",
    "  print(k)\n",
    "  print(v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] g21 ; set units to millimeters g90 ; absolute positioning g1 x3. 939 y3. 838 f1200 g1 x3. 885 y3. 830 f1200 g1 x3. 833 y3. 815 f1200 g1 x3. 783 y3. 795 f1200 g1 x3. 735 y3. 770 f1200 g1 x3. 690 y3. 739 f1200 g1 x3. 649 y3. 704 f1200 g1 x3. 611 y3. 665 f1200 g1 x3. 579 y3. 621 f1200 g1 x3. 551 y3. 575 f1200 g1 x3. 528 y3. 526 f1200 g1 x3. 511 y3. 474 f1200 g1 x3. 499 y3. 421 f1200 g1 x3. 494 y3. 367 f1200 g1 x3. 494 y3. 313 f1200 g1 x3. 500 y3. 259 f1200 g1 x3. 512 y3. 206 f1200 g1 x3. 529 y3. 155 f1200 g1 x3. 552 y3. 106 f1200 g1 x3. 580 y3. 059 f1200 g1 x3. 613 y3. 016 f1200 g1 x3. 650 y2. 977 f1200 g1 x3. 692 y2. 942 f1200 g1 x3. 737 y2. 912 f1200 g1 x3. 785 y2. 887 f1200 g1 x3. 835 y2. 867 f1200 g1 x3. 888 y2. 853 f1200 g1 x3. 941 y2. 844 f1200 g1 x3. 995 y2. 841 f1200 g1 x4. 049 y2. 845 f1200 g1 x4. 103 y2. 854 f1200 g1 x4. 155 y2. 868 f1200 g1 x4. 205 y2. 889 f1200 g1 x4. 253 y2. 914 f1200 g1 x4. 298 y2. 945 f1200 g1 x4. 339 y2. 980 f1200 g1 x4. 376 y3. 020 f1 [SEP]'"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.decode(batch[\"input_ids\"][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can \"denormalize\" the pixel values to get back an image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=224x224>"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "MEAN = np.array([123.675, 116.280, 103.530]) / 255\n",
    "STD = np.array([58.395, 57.120, 57.375]) / 255\n",
    "\n",
    "unnormalized_image = (batch[\"pixel_values\"][0].numpy() * np.array(STD)[:, None, None]) + np.array(MEAN)[:, None, None]\n",
    "unnormalized_image = (unnormalized_image * 255).astype(np.uint8)\n",
    "unnormalized_image = np.moveaxis(unnormalized_image, 0, -1)\n",
    "Image.fromarray(unnormalized_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://huggingface.co/models?other=git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/git-base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dummy forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(8.7084, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model(input_ids=batch[\"input_ids\"],\n",
    "                attention_mask=batch[\"attention_mask\"],\n",
    "                pixel_values=batch[\"pixel_values\"],\n",
    "                labels=batch[\"input_ids\"])\n",
    "outputs.loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Loss: 9.767441749572754\n",
      "Loss: 7.033280849456787\n",
      "Epoch: 1\n",
      "Loss: 7.953902244567871\n",
      "Loss: 5.828039646148682\n",
      "Epoch: 2\n",
      "Loss: 7.273092746734619\n",
      "Loss: 5.1944684982299805\n",
      "Epoch: 3\n",
      "Loss: 6.894998550415039\n",
      "Loss: 4.829487323760986\n",
      "Epoch: 4\n",
      "Loss: 6.566226005554199\n",
      "Loss: 4.605932235717773\n",
      "Epoch: 5\n",
      "Loss: 6.294013977050781\n",
      "Loss: 4.376321315765381\n",
      "Epoch: 6\n",
      "Loss: 6.054511547088623\n",
      "Loss: 4.190847396850586\n",
      "Epoch: 7\n",
      "Loss: 5.875650882720947\n",
      "Loss: 4.033324718475342\n",
      "Epoch: 8\n",
      "Loss: 5.69215202331543\n",
      "Loss: 3.912658214569092\n",
      "Epoch: 9\n",
      "Loss: 5.536532878875732\n",
      "Loss: 3.785839557647705\n",
      "Epoch: 10\n",
      "Loss: 5.376466751098633\n",
      "Loss: 3.661569356918335\n",
      "Epoch: 11\n",
      "Loss: 5.230008602142334\n",
      "Loss: 3.5593068599700928\n",
      "Epoch: 12\n",
      "Loss: 5.079276084899902\n",
      "Loss: 3.462568998336792\n",
      "Epoch: 13\n",
      "Loss: 4.936217308044434\n",
      "Loss: 3.339724063873291\n",
      "Epoch: 14\n",
      "Loss: 4.7989888191223145\n",
      "Loss: 3.2390661239624023\n",
      "Epoch: 15\n",
      "Loss: 4.661665916442871\n",
      "Loss: 3.1433417797088623\n",
      "Epoch: 16\n",
      "Loss: 4.519337177276611\n",
      "Loss: 3.0266098976135254\n",
      "Epoch: 17\n",
      "Loss: 4.381490230560303\n",
      "Loss: 2.9319562911987305\n",
      "Epoch: 18\n",
      "Loss: 4.249319553375244\n",
      "Loss: 2.8453540802001953\n",
      "Epoch: 19\n",
      "Loss: 4.102671146392822\n",
      "Loss: 2.7454164028167725\n",
      "Epoch: 20\n",
      "Loss: 3.969566822052002\n",
      "Loss: 2.6494693756103516\n",
      "Epoch: 21\n",
      "Loss: 3.8314919471740723\n",
      "Loss: 2.5511064529418945\n",
      "Epoch: 22\n",
      "Loss: 3.69362473487854\n",
      "Loss: 2.4569804668426514\n",
      "Epoch: 23\n",
      "Loss: 3.546873092651367\n",
      "Loss: 2.370558738708496\n",
      "Epoch: 24\n",
      "Loss: 3.407302141189575\n",
      "Loss: 2.2674753665924072\n",
      "Epoch: 25\n",
      "Loss: 3.2702765464782715\n",
      "Loss: 2.1648497581481934\n",
      "Epoch: 26\n",
      "Loss: 3.124096155166626\n",
      "Loss: 2.0881521701812744\n",
      "Epoch: 27\n",
      "Loss: 2.9870965480804443\n",
      "Loss: 1.9866390228271484\n",
      "Epoch: 28\n",
      "Loss: 2.851588010787964\n",
      "Loss: 1.8808245658874512\n",
      "Epoch: 29\n",
      "Loss: 2.696773052215576\n",
      "Loss: 1.7830023765563965\n",
      "Epoch: 30\n",
      "Loss: 2.557805061340332\n",
      "Loss: 1.6888530254364014\n",
      "Epoch: 31\n",
      "Loss: 2.4262309074401855\n",
      "Loss: 1.5970664024353027\n",
      "Epoch: 32\n",
      "Loss: 2.2828140258789062\n",
      "Loss: 1.5268882513046265\n",
      "Epoch: 33\n",
      "Loss: 2.1418051719665527\n",
      "Loss: 1.4251397848129272\n",
      "Epoch: 34\n",
      "Loss: 2.0085408687591553\n",
      "Loss: 1.3417882919311523\n",
      "Epoch: 35\n",
      "Loss: 1.8818299770355225\n",
      "Loss: 1.2545416355133057\n",
      "Epoch: 36\n",
      "Loss: 1.7433886528015137\n",
      "Loss: 1.168440818786621\n",
      "Epoch: 37\n",
      "Loss: 1.6181210279464722\n",
      "Loss: 1.0868061780929565\n",
      "Epoch: 38\n",
      "Loss: 1.5028293132781982\n",
      "Loss: 1.0094388723373413\n",
      "Epoch: 39\n",
      "Loss: 1.384745478630066\n",
      "Loss: 0.9346057772636414\n",
      "Epoch: 40\n",
      "Loss: 1.2664010524749756\n",
      "Loss: 0.8638426065444946\n",
      "Epoch: 41\n",
      "Loss: 1.1676198244094849\n",
      "Loss: 0.7984869480133057\n",
      "Epoch: 42\n",
      "Loss: 1.066847562789917\n",
      "Loss: 0.7287752032279968\n",
      "Epoch: 43\n",
      "Loss: 0.9832426905632019\n",
      "Loss: 0.6670323014259338\n",
      "Epoch: 44\n",
      "Loss: 0.8920931220054626\n",
      "Loss: 0.6102700233459473\n",
      "Epoch: 45\n",
      "Loss: 0.8153530359268188\n",
      "Loss: 0.5577244758605957\n",
      "Epoch: 46\n",
      "Loss: 0.7524363398551941\n",
      "Loss: 0.5178617835044861\n",
      "Epoch: 47\n",
      "Loss: 0.6917350888252258\n",
      "Loss: 0.47122278809547424\n",
      "Epoch: 48\n",
      "Loss: 0.6436734795570374\n",
      "Loss: 0.42971786856651306\n",
      "Epoch: 49\n",
      "Loss: 0.588463306427002\n",
      "Loss: 0.3908807933330536\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "model.train()\n",
    "\n",
    "for epoch in range(50):\n",
    "  print(\"Epoch:\", epoch)\n",
    "  for idx, batch in enumerate(train_dataloader):\n",
    "    input_ids = batch.pop(\"input_ids\").to(device)\n",
    "    pixel_values = batch.pop(\"pixel_values\").to(device)\n",
    "\n",
    "    outputs = model(input_ids=input_ids,\n",
    "                    pixel_values=pixel_values,\n",
    "                    labels=input_ids)\n",
    "\n",
    "    loss = outputs.loss\n",
    "\n",
    "    print(\"Loss:\", loss.item())\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(test_dataset, shuffle=True, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids torch.Size([2, 512])\n",
      "attention_mask torch.Size([2, 512])\n",
      "pixel_values torch.Size([2, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(test_dataloader))\n",
    "for k,v in batch.items():\n",
    "  print(k,v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x338395690>"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iter = iter(test_dataloader)\n",
    "first_batch = next(data_iter)\n",
    "first_example = first_batch['pixel_values'][0]  # Adjust indexing based on how your data is structured\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCABAAEADASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwD3+iiigAooooAKKKKACiiigAooooAKKKKACivN/F+q6jqbeMbez1Sexs/D2lM7R2zrHNPdNE0qMXVvMWNAExgLvYuCWVSD6RQAUUUUAFFFFABRRRQByfizwbN4geefT9Xk024urKTTrzdEZori3ZXABj3Lh0Z2ZXBBGWB3A4rU0bTNUtLy+vNW1n7fNc+WscMMHkQW6IDwiFnO5izFmLHPyjgKK2KKACiiigAooooAKKKKACiiigAooooA/9k=",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEAAAABACAIAAAAlC+aJAAABC0lEQVR4Ae2YQQ7CMAwECf//cwgEVVW3h0rrbFVpeioWsb0z4ULrvb+e/LyfvPx3dwLcbRADGDAJcIVMgPZxDNgIzQYYMAHaxzFgIzQbYMAEaB/HgI3QbIABE6B9HAM2QrMBBkyA9vEbDLTfY2/+bxANMFev/S8wF2BsP6DVbj8a5gKs2D4XYOIf88qfnIHyyzNZ5AIskhAKsAj/kBAKMHWfSjgtXv+p5AJMCYd1Dx+v7719s62Tu83Yv+jG5gLpAPswJe+5K1SyrjYhgDLJVjCQ5a3TMKBMshUMZHnrNAwok2wFA1neOg0DyiRbwUCWt07DgDLJVjCQ5a3TMKBMshUMZHnrNAwok2zlA9bZHoQjkTl7AAAAAElFTkSuQmCC",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=64x64>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load image\n",
    "example = test_dataset[0]\n",
    "width, height = image.size\n",
    "display(image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g21 ; set units to millimeters g90 ; absolute positioning g1 x3. 939 y3. 838 f1200 g1 x3. 835 y3. 830 f1200 g1 x3. 833 y\n"
     ]
    }
   ],
   "source": [
    "# prepare image for the model\n",
    "inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "pixel_values = inputs.pixel_values\n",
    "\n",
    "generated_ids = model.generate(pixel_values=pixel_values, max_length=50)\n",
    "generated_caption = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(generated_caption)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
